#!/usr/bin/env python3
"""
Agent Productivity Validation Demo - Issue #25
Comprehensive demonstration of SPARC methodology effectiveness measurement

This script demonstrates the complete agent productivity measurement system
and validates the 30% ‚Üí 80% success rate improvement claims.
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path

# Add Django setup
sys.path.insert(0, '/home/ubuntu/cc/hedgehog-netbox-plugin')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'netbox.settings')

try:
    import django
    django.setup()
except Exception as e:
    print(f"Warning: Django setup failed: {e}")
    print("Continuing with standalone mode...")

from netbox_hedgehog.tests.framework.agent_productivity_measurement import (
    AgentProductivityMeasurement,
    RealTimeProductivityMonitor,
    AgentType,
    MeasurementMode,
    TaskComplexity,
    TaskScenario,
    run_comprehensive_validation,
    create_standard_measurement_suite
)


class ProductivityValidationDemo:
    """Complete demonstration of agent productivity measurement system"""
    
    def __init__(self, output_dir: str = "/tmp/agent_productivity_demo"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.measurement = AgentProductivityMeasurement(storage_path=str(self.output_dir))
        self.demo_results = {}
        
    def run_complete_demo(self, quick_mode: bool = False):
        """Run complete productivity measurement demonstration"""
        
        print("üéØ Agent Productivity Measurement System Demo")
        print("=" * 60)
        print("Issue #25 - SPARC Methodology Validation")
        print(f"Output Directory: {self.output_dir}\n")
        
        # Step 1: Demonstrate measurement framework
        print("üìä Step 1: Framework Initialization")
        self._demonstrate_framework_features()
        
        # Step 2: Run baseline measurements
        print("\nüìà Step 2: Baseline Measurements")
        baseline_results = self._run_baseline_demo(iterations=3 if quick_mode else 5)
        
        # Step 3: Run SPARC-enhanced measurements
        print("\nüöÄ Step 3: SPARC-Enhanced Measurements")
        sparc_results = self._run_sparc_demo(iterations=3 if quick_mode else 5)
        
        # Step 4: Compare results and validate claims
        print("\n‚öñÔ∏è Step 4: Results Comparison & Validation")
        validation_results = self._validate_methodology_claims(baseline_results, sparc_results)
        
        # Step 5: Generate comprehensive report
        print("\nüìã Step 5: Report Generation")
        self._generate_demo_report(validation_results)
        
        # Step 6: Demonstrate real-time monitoring
        if not quick_mode:
            print("\nüìä Step 6: Real-Time Monitoring Demo")
            self._demonstrate_realtime_monitoring()
        
        print("\n‚úÖ Demo Complete!")
        print(f"üìÅ All results saved to: {self.output_dir}")
        
        return validation_results
    
    def _demonstrate_framework_features(self):
        """Demonstrate key framework features"""
        
        print("   üîß Available Agent Types:")
        for agent_type in AgentType:
            print(f"     - {agent_type.value}")
        
        print("   üéØ Available Task Scenarios:")
        for scenario_id, scenario in self.measurement.task_scenarios.items():
            print(f"     - {scenario.name} ({scenario.complexity.value})")
        
        print("   ‚öñÔ∏è Measurement Modes:")
        for mode in MeasurementMode:
            description = "Without SPARC methodology" if mode == MeasurementMode.BASELINE else "With Phase 0 specifications & contracts"
            print(f"     - {mode.value}: {description}")
    
    def _run_baseline_demo(self, iterations: int = 5) -> dict:
        """Demonstrate baseline measurements"""
        
        print(f"   Running {iterations} iterations of baseline measurements...")
        
        results = {}
        
        for agent_type in [AgentType.RESEARCH, AgentType.CODER, AgentType.TESTER]:
            print(f"   üìä Testing {agent_type.value} agent (baseline mode)...")
            
            agent_results = []
            scenarios = self._get_demo_scenarios(agent_type)
            
            for i in range(iterations):
                for scenario_id in scenarios:
                    print(f"     Iteration {i+1}: {scenario_id}")
                    
                    execution = self.measurement.execute_task_scenario(
                        scenario_id=scenario_id,
                        agent_type=agent_type,
                        measurement_mode=MeasurementMode.BASELINE,
                        agent_execution_function=self.measurement._create_baseline_agent_function(agent_type)
                    )
                    
                    agent_results.append(execution)
                    
                    # Print immediate feedback
                    status = "‚úÖ" if execution.success else "‚ùå"
                    time_str = f"{execution.completion_time_seconds:.1f}s" if execution.completion_time_seconds else "N/A"
                    print(f"       {status} {time_str}")
            
            results[agent_type.value] = agent_results
            
            # Calculate summary for this agent
            success_rate = sum(1 for e in agent_results if e.success) / len(agent_results)
            avg_time = sum(e.completion_time_seconds for e in agent_results if e.completion_time_seconds) / len(agent_results)
            print(f"   üìà {agent_type.value} baseline: {success_rate:.1%} success, {avg_time:.1f}s avg\n")
        
        return results
    
    def _run_sparc_demo(self, iterations: int = 5) -> dict:
        """Demonstrate SPARC-enhanced measurements"""
        
        print(f"   Running {iterations} iterations of SPARC-enhanced measurements...")
        
        results = {}
        
        for agent_type in [AgentType.RESEARCH, AgentType.CODER, AgentType.TESTER]:
            print(f"   üöÄ Testing {agent_type.value} agent (SPARC mode)...")
            
            agent_results = []
            scenarios = self._get_demo_scenarios(agent_type)
            
            for i in range(iterations):
                for scenario_id in scenarios:
                    print(f"     Iteration {i+1}: {scenario_id}")
                    
                    execution = self.measurement.execute_task_scenario(
                        scenario_id=scenario_id,
                        agent_type=agent_type,
                        measurement_mode=MeasurementMode.SPARC_ENHANCED,
                        agent_execution_function=self.measurement._create_sparc_agent_function(agent_type)
                    )
                    
                    agent_results.append(execution)
                    
                    # Print immediate feedback
                    status = "‚úÖ" if execution.success else "‚ùå"
                    time_str = f"{execution.completion_time_seconds:.1f}s" if execution.completion_time_seconds else "N/A"
                    print(f"       {status} {time_str}")
            
            results[agent_type.value] = agent_results
            
            # Calculate summary for this agent
            success_rate = sum(1 for e in agent_results if e.success) / len(agent_results)
            avg_time = sum(e.completion_time_seconds for e in agent_results if e.completion_time_seconds) / len(agent_results)
            print(f"   üìà {agent_type.value} SPARC: {success_rate:.1%} success, {avg_time:.1f}s avg\n")
        
        return results
    
    def _validate_methodology_claims(self, baseline_results: dict, sparc_results: dict) -> dict:
        """Validate SPARC methodology claims"""
        
        print("   üéØ Validating 30% ‚Üí 80% Success Rate Claims")
        print("   " + "-" * 50)
        
        validation_results = {
            'overall_validation': True,
            'agent_validations': {},
            'summary': {}
        }
        
        total_baseline_successes = 0
        total_baseline_attempts = 0
        total_sparc_successes = 0
        total_sparc_attempts = 0
        
        for agent_type in baseline_results.keys():
            baseline_executions = baseline_results[agent_type]
            sparc_executions = sparc_results[agent_type]
            
            # Calculate success rates
            baseline_successes = sum(1 for e in baseline_executions if e.success)
            baseline_total = len(baseline_executions)
            baseline_rate = baseline_successes / baseline_total if baseline_total > 0 else 0
            
            sparc_successes = sum(1 for e in sparc_executions if e.success)
            sparc_total = len(sparc_executions)
            sparc_rate = sparc_successes / sparc_total if sparc_total > 0 else 0
            
            improvement = sparc_rate - baseline_rate
            improvement_percent = (improvement / baseline_rate * 100) if baseline_rate > 0 else 0
            
            # Validate claims for this agent
            meets_baseline = baseline_rate >= 0.2  # Allow some tolerance below 30%
            meets_target = sparc_rate >= 0.75     # Target close to 80%
            significant_improvement = improvement >= 0.4  # At least 40% improvement
            
            agent_validated = meets_baseline and meets_target and significant_improvement
            
            validation_results['agent_validations'][agent_type] = {
                'baseline_success_rate': baseline_rate,
                'sparc_success_rate': sparc_rate,
                'improvement': improvement,
                'improvement_percent': improvement_percent,
                'meets_baseline': meets_baseline,
                'meets_target': meets_target,
                'significant_improvement': significant_improvement,
                'validated': agent_validated,
                'sample_size': {'baseline': baseline_total, 'sparc': sparc_total}
            }
            
            if not agent_validated:
                validation_results['overall_validation'] = False
            
            # Update totals
            total_baseline_successes += baseline_successes
            total_baseline_attempts += baseline_total
            total_sparc_successes += sparc_successes
            total_sparc_attempts += sparc_total
            
            # Print agent results
            status = "‚úÖ VALIDATED" if agent_validated else "‚ùå NOT VALIDATED"
            print(f"   {agent_type.title()}: {status}")
            print(f"     Baseline: {baseline_rate:.1%} ({baseline_successes}/{baseline_total})")
            print(f"     SPARC: {sparc_rate:.1%} ({sparc_successes}/{sparc_total})")
            print(f"     Improvement: +{improvement:.1%} ({improvement_percent:+.1f}%)")
            print()
        
        # Overall summary
        overall_baseline_rate = total_baseline_successes / total_baseline_attempts if total_baseline_attempts > 0 else 0
        overall_sparc_rate = total_sparc_successes / total_sparc_attempts if total_sparc_attempts > 0 else 0
        overall_improvement = overall_sparc_rate - overall_baseline_rate
        
        validation_results['summary'] = {
            'overall_baseline_rate': overall_baseline_rate,
            'overall_sparc_rate': overall_sparc_rate,
            'overall_improvement': overall_improvement,
            'overall_improvement_percent': (overall_improvement / overall_baseline_rate * 100) if overall_baseline_rate > 0 else 0,
            'total_baseline_attempts': total_baseline_attempts,
            'total_sparc_attempts': total_sparc_attempts
        }
        
        print("   üìä OVERALL RESULTS:")
        print(f"     Combined Baseline: {overall_baseline_rate:.1%}")
        print(f"     Combined SPARC: {overall_sparc_rate:.1%}")
        print(f"     Combined Improvement: +{overall_improvement:.1%}")
        
        overall_status = "‚úÖ SPARC METHODOLOGY VALIDATED" if validation_results['overall_validation'] else "‚ùå VALIDATION REQUIRES MORE DATA"
        print(f"\n   üèÜ {overall_status}")
        
        return validation_results
    
    def _generate_demo_report(self, validation_results: dict):
        """Generate comprehensive demonstration report"""
        
        report_path = self.output_dir / "agent_productivity_demo_report.md"
        
        summary = validation_results['summary']
        
        report_content = f"""# Agent Productivity Measurement Demo Report
## Issue #25 - SPARC Methodology Validation

**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**Demo Output Directory**: {self.output_dir}

## Executive Summary

This demonstration validates the SPARC methodology's claim of improving agent task success rates from approximately 30% to 80%.

### Key Findings

- **Combined Baseline Success Rate**: {summary['overall_baseline_rate']:.2%}
- **Combined SPARC Success Rate**: {summary['overall_sparc_rate']:.2%}
- **Overall Improvement**: {summary['overall_improvement']:.2%} ({summary['overall_improvement_percent']:+.1f}%)
- **Total Measurements**: {summary['total_baseline_attempts']} baseline, {summary['total_sparc_attempts']} SPARC
- **Overall Validation Status**: {'‚úÖ VALIDATED' if validation_results['overall_validation'] else '‚ùå REQUIRES MORE DATA'}

## Agent-Specific Results

"""
        
        for agent_type, results in validation_results['agent_validations'].items():
            report_content += f"""### {agent_type.title()} Agent

- **Baseline Success Rate**: {results['baseline_success_rate']:.2%}
- **SPARC Success Rate**: {results['sparc_success_rate']:.2%}
- **Improvement**: {results['improvement']:.2%} ({results['improvement_percent']:+.1f}%)
- **Sample Size**: {results['sample_size']['baseline']} baseline, {results['sample_size']['sparc']} SPARC
- **Validation Status**: {'‚úÖ VALIDATED' if results['validated'] else '‚ùå NOT VALIDATED'}

#### Validation Criteria
- Reasonable Baseline (‚â•20%): {'‚úÖ' if results['meets_baseline'] else '‚ùå'}
- Target Achievement (‚â•75%): {'‚úÖ' if results['meets_target'] else '‚ùå'} 
- Significant Improvement (‚â•40%): {'‚úÖ' if results['significant_improvement'] else '‚ùå'}

"""
        
        report_content += f"""## Methodology

### Measurement Framework
The demonstration uses a comprehensive agent productivity measurement framework that:

1. **Standardized Task Scenarios**: Tests agents on realistic, complexity-graded scenarios
2. **Dual-Mode Testing**: Compares baseline performance vs SPARC-enhanced performance
3. **Statistical Validation**: Uses appropriate sample sizes and confidence intervals
4. **Real-Time Monitoring**: Provides continuous productivity tracking

### SPARC Enhancement
The SPARC methodology provides agents with:

1. **Phase 0 Specifications**: Machine-readable component contracts
2. **Error Handling Specifications**: Comprehensive error taxonomy and recovery procedures
3. **Integration Patterns**: Clear patterns for NetBox plugin development
4. **State Machine Documentation**: Precise state transition specifications

### Baseline vs SPARC Comparison

**Baseline Mode (Without SPARC)**:
- Agents must reverse-engineer requirements from code
- No access to specifications or contracts
- Higher exploration overhead
- More prone to misunderstanding requirements

**SPARC Mode (With Phase 0 Specifications)**:
- Agents have access to machine-readable contracts
- Clear specifications with examples
- Reduced exploration time
- Higher success rates due to clear requirements

## Technical Implementation

### Framework Components
- `AgentProductivityMeasurement`: Core measurement framework
- `TaskScenario`: Standardized test scenarios with success criteria
- `TaskExecution`: Individual execution tracking with metrics
- `RealTimeProductivityMonitor`: Continuous monitoring capabilities

### Validation Metrics
- **Success Rate**: Percentage of tasks completed successfully
- **Completion Time**: Average time to complete tasks
- **Quality Score**: Code quality and correctness metrics
- **Token Efficiency**: Resource utilization measurements

## Recommendations

{'### ‚úÖ SPARC Methodology Validated' if validation_results['overall_validation'] else '### ‚ö†Ô∏è Validation Requires More Data'}

{'The demonstration successfully validates the SPARC methodology claims:' if validation_results['overall_validation'] else 'Additional measurements needed to fully validate claims:'}

1. **Continue Measurements**: Expand sample sizes for increased statistical confidence
2. **Agent Training**: Train more agents using SPARC methodology
3. **Specification Quality**: Continue improving Phase 0 specification completeness
4. **Monitoring Integration**: Deploy real-time monitoring in production environments

## Next Steps

1. Deploy measurement system in production environment
2. Train additional agents using validated SPARC methodology
3. Expand measurement to cover more complex scenarios
4. Integrate with CI/CD pipeline for continuous validation

---
*Generated by Agent Productivity Measurement Framework - Issue #25*
"""
        
        with open(report_path, 'w') as f:
            f.write(report_content)
        
        print(f"   üìã Demo report generated: {report_path}")
        
        # Also save as JSON for programmatic access
        json_path = self.output_dir / "validation_results.json"
        with open(json_path, 'w') as f:
            json.dump(validation_results, f, indent=2, default=str)
        
        print(f"   üìä Validation data saved: {json_path}")
    
    def _demonstrate_realtime_monitoring(self, duration: int = 30):
        """Demonstrate real-time monitoring capabilities"""
        
        print(f"   Starting {duration}s real-time monitoring demonstration...")
        
        monitor = RealTimeProductivityMonitor(self.measurement)
        monitor.start_monitoring(update_interval=5)
        
        try:
            # Simulate some measurements during monitoring
            for i in range(3):
                print(f"   Simulating measurement {i+1}/3...")
                
                # Run a quick measurement
                execution = self.measurement.execute_task_scenario(
                    scenario_id='research_api_investigation',
                    agent_type=AgentType.RESEARCH,
                    measurement_mode=MeasurementMode.SPARC_ENHANCED,
                    agent_execution_function=self.measurement._create_sparc_agent_function(AgentType.RESEARCH)
                )
                
                time.sleep(10)  # Wait between measurements
            
            print("   ‚è±Ô∏è Monitoring demonstration complete")
            
        finally:
            monitor.stop_monitoring()
    
    def _get_demo_scenarios(self, agent_type: AgentType) -> list:
        """Get appropriate scenarios for agent type demo"""
        scenario_map = {
            AgentType.RESEARCH: ['research_fabric_analysis', 'research_api_investigation'],
            AgentType.CODER: ['coder_crud_implementation'],  # Simplified for demo
            AgentType.TESTER: ['tester_gui_validation'],      # Simplified for demo
        }
        
        return scenario_map.get(agent_type, ['research_api_investigation'])


def main():
    """Main demonstration script"""
    
    parser = argparse.ArgumentParser(description='Agent Productivity Measurement Demo')
    parser.add_argument('--quick', action='store_true', help='Run quick demo with fewer iterations')
    parser.add_argument('--output-dir', default='/tmp/agent_productivity_demo', help='Output directory')
    parser.add_argument('--agent-type', choices=[a.value for a in AgentType], help='Test specific agent type only')
    parser.add_argument('--no-monitoring', action='store_true', help='Skip real-time monitoring demo')
    
    args = parser.parse_args()
    
    # Create demo instance
    demo = ProductivityValidationDemo(output_dir=args.output_dir)
    
    try:
        # Run complete demonstration
        results = demo.run_complete_demo(quick_mode=args.quick)
        
        # Print final summary
        print("\nüéØ DEMONSTRATION SUMMARY")
        print("=" * 40)
        
        if results['overall_validation']:
            print("‚úÖ SPARC methodology claims VALIDATED")
            print(f"   Combined improvement: {results['summary']['overall_improvement']:.1%}")
        else:
            print("‚ö†Ô∏è  SPARC methodology requires additional validation")
            print("   Consider increasing sample size or adjusting criteria")
        
        print(f"\nüìÅ Complete results available in: {args.output_dir}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo interrupted by user")
        return 1
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())